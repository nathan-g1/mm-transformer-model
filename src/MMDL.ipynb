{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install seaborn\n",
    "LIMIT_BATCH_SIZE = 3900\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "from rasterio.windows import Window\n",
    "from rasterio.transform import xy\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "import random\n",
    "import pandas as pd\n",
    "from rembg import remove\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "## Fusion imports\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Dense, Concatenate, Input, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.utils import normalize\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout\n",
    "#from keras.layers import CuDNNLSTM\n",
    "from keras.layers import Dense, Concatenate\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nathan/Documents/Files/Grad/UTSA/Courses/Thesis/mm-transformer-model/S2_tif\n"
     ]
    }
   ],
   "source": [
    "path1 = os.path.abspath('../')\n",
    "path_ATL03 = os.path.join(path1, \"IS2_LSTM\")\n",
    "path_s2 = os.path.join(path1, \"S2_tif\")\n",
    "path_csv = os.path.join(path1, \"csv\")\n",
    "path_before_training = os.path.join(path1, \"Before_training\")\n",
    "print(path_s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tensor_identity(shape):\n",
    "    # Initialize an empty tensor of zeros\n",
    "    samples, timesteps, features = shape\n",
    "    identity_tensor = np.zeros((samples, timesteps, features))\n",
    "\n",
    "    # Fill in the pseudo-identity matrices for each slice\n",
    "    for i in range(samples):\n",
    "        # Place ones along the diagonal, limiting to the minimum dimension\n",
    "        np.fill_diagonal(identity_tensor[i], 1)\n",
    "\n",
    "    return identity_tensor\n",
    "def create_zero_padded_image_tensor(shape):\n",
    "    samples, height, width, channels = shape\n",
    "    zero_padded_tensor = np.zeros((samples, height, width, channels))\n",
    "    return zero_padded_tensor\n",
    "\n",
    "def get_lstm_model():\n",
    "    units = 32\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, activation='elu', input_shape=(5, 4)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(16, activation='elu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "\n",
    "    sample_weight=np.array([0.10, 0.35, 0.65])\n",
    "\n",
    "    # Define the optimizer with a specific learning rate\n",
    "    learning_rate = 0.005\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    focal_loss = tf.keras.losses.CategoricalFocalCrossentropy(\n",
    "        alpha=sample_weight,\n",
    "        gamma=2.0,\n",
    "        from_logits=False,\n",
    "        label_smoothing=0.0,\n",
    "        axis=-1,\n",
    "        reduction='sum_over_batch_size',\n",
    "        name='categorical_focal_crossentropy'\n",
    "    )\n",
    "    model.compile(optimizer=optimizer, loss=focal_loss, metrics=['accuracy', f1_m,precision_m, recall_m])\n",
    "    return model\n",
    "\n",
    "\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "# lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    epochs=50,\n",
    "    batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Train the model with validation\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    begin = time.time()\n",
    "    history = model.fit(\n",
    "        X_train,  # Input data for the single modality\n",
    "        y_train,  # Target labels\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping, lr_scheduler]\n",
    "    )\n",
    "    end = time.time()\n",
    "    print(f\"The training time is {end - begin} seconds.\")\n",
    "    return history\n",
    "# Evaluates a single modality model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate model and print classification metrics\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "    num_classes = len(np.unique(y_test_classes))\n",
    "    class_names = ['Class ' + str(i) for i in range(num_classes)]\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test_classes, y_pred_classes, target_names=class_names))\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "    # Normalize the confusion matrix\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_percentage, annot=True, fmt='.2f', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    # Calculate and return metrics\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "# Evaluates a fusion model\n",
    "def evaluate_fusion_model(model, X_test_unet, X_test_lstm, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate model and print classification metrics\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    y_pred = model.predict([X_test_unet, X_test_lstm])\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "    # class_names = ['Thick Ice ', 'Thin Ice', 'Open Water']\n",
    "    num_classes = len(np.unique(y_test_classes))\n",
    "    class_names = ['Class ' + str(i) for i in range(num_classes)]\n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test_classes, y_pred_classes, target_names=class_names))\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "    # Normalize the confusion matrix\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_percentage, annot=True, fmt='.2f', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    # Calculate and return metrics\n",
    "    results = model.evaluate([X_test_unet, X_test_lstm], y_test, verbose=1)\n",
    "    test_loss = results[0]\n",
    "    test_accuracy = results[1]\n",
    "    print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "\n",
    "def train_fusion_model(\n",
    "    model,\n",
    "    X_train_unet,\n",
    "    X_train_lstm,\n",
    "    y_train,\n",
    "    X_val_unet,\n",
    "    X_val_lstm,\n",
    "    y_val,\n",
    "    epochs=50,\n",
    "    batch_size=BATCH_SIZE\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Train the fusion model with validation\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # 3. Use a custom learning rate and optimizer\n",
    "    lr_scheduler = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "    # 4. Implement improved callbacks\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        mode='min'\n",
    "    )\n",
    "\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        'best_mlf_model.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "    begin = time.time()\n",
    "    history = model.fit(\n",
    "        [X_train_unet, X_train_lstm],  # Input data for both branches\n",
    "        y_train,                        # Target labels\n",
    "        validation_data=([X_val_unet, X_val_lstm], y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping, lr_scheduler, checkpoint],\n",
    "        class_weight=None  # Add class weights if dataset is imbalanced\n",
    "    )\n",
    "    end = time.time()\n",
    "    print(f\"The training time is {end - begin} seconds.\")\n",
    "    return history\n",
    "\n",
    "# Function to plot training history\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot training and validation metrics\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax1.set_title('Model Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot loss\n",
    "    ax2.plot(history.history['loss'], label='Training Loss')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax2.set_title('Model Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # save the figure to a file. Grab model name from history\n",
    "    name = history.model.name\n",
    "    fig.savefig(name + '_training_history.png')\n",
    "    return fig\n",
    "\n",
    "def color_segmentation(img):\n",
    "    # Get a \"mask\" over the image for each pixel\n",
    "    # if a pixel's color is between the lower and upper white, its mask is 1\n",
    "    # Otherwise, the pixel's mask is 0\n",
    "    hsv_img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "    \n",
    "    lower_ice = (0, 0, 205)#(127, 7, 94) #increase v to specify ow\n",
    "    upper_ice = (185, 255, 255)#(147, 53, 232) #increase h to specify si\n",
    "    mask_ice = cv2.inRange(hsv_img, lower_ice, upper_ice)\n",
    "    \n",
    "    lower_tice = (0, 0, 31)#(127, 7, 94) #increase v to specify ow\n",
    "    upper_tice = (185, 255, 204)#(147, 53, 232) #increase h to specify si\n",
    "    mask_tice = cv2.inRange(hsv_img, lower_tice, upper_tice)\n",
    "    \n",
    "    lower_water = (0, 0, 0)#(127, 7, 94) #increase v to specify ow\n",
    "    upper_water = (185, 255, 30)#(147, 53, 232) #increase h to specify si\n",
    "    mask_water = cv2.inRange(hsv_img, lower_water, upper_water)\n",
    "    \n",
    "    # duplicate the image\n",
    "    seg_img = img.copy()\n",
    "    #color each masked portion\n",
    "    seg_img[mask_ice == 255] = [255, 0, 0]\n",
    "    seg_img[mask_tice == 255] = [0, 0, 255]\n",
    "    seg_img[mask_water == 255] = [0, 255, 0]\n",
    "    \n",
    "    #seg_img = cv2.cvtColor(seg_img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    return seg_img\n",
    "\n",
    "def shadow_cloud_removal(ori):\n",
    "\n",
    "    ### seperate open water\n",
    "    lower_water = (0, 0, 0)\n",
    "    upper_water = (185, 255, 30)\n",
    "    hsv_img = cv2.cvtColor(ori, cv2.COLOR_RGB2HSV)\n",
    "    mask_water = (cv2.inRange(hsv_img, lower_water, upper_water))\n",
    "\n",
    "    # duplicate the image\n",
    "    without_water_img = ori.copy()\n",
    "    without_water_img[mask_water == 255] = [255, 255, 255]\n",
    "    #plot_image(ori, water_img)\n",
    "\n",
    "    #img = cv2.imread('s2_vis_2.png',cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.cvtColor(without_water_img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    dilated_img = cv2.dilate(img, np.ones((7,7), np.uint8))\n",
    "    bg_img = cv2.medianBlur(dilated_img, 155)\n",
    "    diff_img = 255 - cv2.absdiff(img, bg_img)\n",
    "\n",
    "    ret2, outs2 = cv2.threshold(src = diff_img, thresh = 0, maxval = 255, type = cv2.THRESH_OTSU+cv2.THRESH_BINARY)\n",
    "    diff_img2 = cv2.bitwise_and(diff_img, outs2)\n",
    "\n",
    "    norm_img = cv2.normalize(diff_img2, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)\n",
    "    _, thr_img = cv2.threshold(norm_img, 235, 0, cv2.THRESH_TRUNC)\n",
    "    thr_img = cv2.normalize(thr_img, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)\n",
    "\n",
    "    ### seperate thin and old ice\n",
    "    old_thin_ice = cv2.cvtColor(thr_img,cv2.COLOR_GRAY2RGB)\n",
    "    hsv_img = cv2.cvtColor(old_thin_ice, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "    lower_tice = (0, 0, 0)\n",
    "    upper_tice = (185, 255, 204)\n",
    "    mask_tice = cv2.inRange(hsv_img, lower_tice, upper_tice)\n",
    "\n",
    "    lower_ice = (0, 0, 205)\n",
    "    upper_ice = (185, 255, 255)\n",
    "    mask_ice = cv2.inRange(hsv_img, lower_ice, upper_ice)\n",
    "    mask_ice = cv2.bitwise_xor(mask_water, mask_ice)\n",
    "    #plot_image3(mask_water, mask_tice, mask_ice)\n",
    "\n",
    "    # duplicate the image\n",
    "    shadow_free = old_thin_ice.copy()\n",
    "    #color each masked portion\n",
    "    shadow_free[mask_ice == 255] = [255, 255, 255]\n",
    "    shadow_free[mask_tice == 255] = [155, 155, 155]\n",
    "    shadow_free[mask_water == 255] = [0, 0, 0]\n",
    "    shadow_free = cv2.cvtColor(shadow_free, cv2.COLOR_BGR2RGB)\n",
    "    #plot_image(shadow_free, ori)\n",
    "\n",
    "    #segmentation\n",
    "    #img = cv2.cvtColor(water_img,cv2.COLOR_GRAY2RGB)\n",
    "    seg_img = color_segmentation(ori)\n",
    "    #plot_image(ori, seg_img, title_1 = \"Original Image\", title_2 = \"Segmented original image\")\n",
    "\n",
    "    #final = cv2.cvtColor(shadow_free,cv2.COLOR_GRAY2RGB)\n",
    "    seg_res = color_segmentation(shadow_free)\n",
    "    #gray = cv2.cvtColor(seg_res,cv2.COLOR_RGB2GRAY)\n",
    "    #plot_image(shadow_free, seg_res, title_1 = \"Shadow free Image\", title_2 = \"Segmented shadow free image\")\n",
    "    \n",
    "    #return seg_img, shadow_free, seg_res\n",
    "    return seg_res\n",
    "\n",
    "\n",
    "def rgba2rgb( rgba, background=(0,0,0)):\n",
    "    row, col, ch = rgba.shape\n",
    "    if ch == 3:\n",
    "        return rgba\n",
    "    assert ch == 4, 'RGBA image has 4 channels.'\n",
    "    rgb = np.zeros( (row, col, 3), dtype='float32' )\n",
    "    r, g, b, a = rgba[:,:,0], rgba[:,:,1], rgba[:,:,2], rgba[:,:,3]\n",
    "    a = np.asarray( a, dtype='float32' ) / 255.0\n",
    "    R, G, B = background\n",
    "    rgb[:,:,0] = r * a + (1.0 - a) * R\n",
    "    rgb[:,:,1] = g * a + (1.0 - a) * G\n",
    "    rgb[:,:,2] = b * a + (1.0 - a) * B\n",
    "    return np.asarray( rgb, dtype='uint8' )\n",
    "\n",
    "def plot_image(image_1, image_2,title_1=\"Orignal\",title_2=\"New Image\"):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(cv2.cvtColor(image_1, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(title_1)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(cv2.cvtColor(image_2, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(title_2)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "tiff_files = ['s2_vis_00_20191103T183459_20191103T183502_T05CMR.tif', 's2_vis_01_20191103T183459_20191103T183502_T05CMS.tif']\n",
    "\n",
    "\n",
    "# Get list of TIFF files\n",
    "tiff_files = sorted(glob.glob(os.path.join(path_s2, \"*.tif\")))\n",
    "\n",
    "# Get list of CSV files\n",
    "csv_files = sorted(glob.glob(os.path.join(path_csv, \"*.csv\")))\n",
    "\n",
    "# Select two random CSV files\n",
    "selected_csv_files = random.sample(csv_files, 2)\n",
    "\n",
    "# ATL03 to S2 mapping file\n",
    "atl03_to_s2_mapping_file = pd.read_csv(os.path.join(path1, \"ATL03_S2_2019-11-01_2019-11-30_ross.csv\"))\n",
    "\n",
    "# Corrected ATL03 to S2 mapping file\n",
    "cleaned_atl03_to_s2_mapping_file = pd.read_csv(os.path.join(path1, \"clean\", \"cleaned_data_indexed.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show 32x32 Grid Lines (Work on first two files only)\n",
    "def show_32x32_grid_lines_on_tif_files():\n",
    "    for file in tiff_files[:2]:\n",
    "        with rasterio.open(file) as img:\n",
    "            width, height = img.width, img.height\n",
    "            transform = img.transform\n",
    "\n",
    "            # Generate grid lines\n",
    "            x_coords = np.linspace(0, width, 33)\n",
    "            y_coords = np.linspace(0, height, 33)\n",
    "\n",
    "            # Plot grid lines\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.imshow(img.read(1), cmap='gray')\n",
    "            for x in x_coords:\n",
    "                plt.axvline(x, color='red', linewidth=0.5)\n",
    "            for y in y_coords:\n",
    "                plt.axhline(y, color='red', linewidth=0.5)\n",
    "            plt.title(f'Grid Lines for {os.path.basename(file)}')\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_csv_if_exists(file):\n",
    "    print(file)\n",
    "    if os.path.exists(file) and os.path.isfile(file):\n",
    "        return pd.read_csv(file)\n",
    "    return None\n",
    "\n",
    "# Function to get coordinates list\n",
    "def get_coordinates_list(file):\n",
    "    # Extract the date in YYYYMMDD format from the filename\n",
    "    date = os.path.basename(file).split(\"_\")[3].split(\"T\")[0]\n",
    "    dataset = cleaned_atl03_to_s2_mapping_file\n",
    "    dataset['transformed_date'] = dataset['date'] // 1000000\n",
    "    # Filter by date and get all unique rows with their original indices\n",
    "    # df = dataset[dataset['date'] == int(date)].drop_duplicates(subset=['index', 'lat', 'lon'])\n",
    "    df = dataset[dataset['transformed_date'] == int(date)][['index', 'lat', 'lon']].drop_duplicates(subset=['lat', 'lon'])\n",
    "\n",
    "    # Create a list of coordinates with their original indices\n",
    "    coordinates_with_indices = list(zip(df['index'], df['lat'], df['lon']))\n",
    "\n",
    "    return coordinates_with_indices  # List of (lat, lon, index) tuples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images, Grids, coordinates, patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patches_from_coordinates(file):\n",
    "    img = rasterio.open(file, crs='EPSG:3976')\n",
    "    raw = img.read(1)\n",
    "    raw[raw < 0] = 0\n",
    "\n",
    "    ori = img.read()\n",
    "    ori[ori < 0] = 0\n",
    "    ori = ori.swapaxes(0, 1)\n",
    "    ori = ori.swapaxes(1, 2)\n",
    "\n",
    "    img_array = ori\n",
    "    img_array[img_array < 0] = 0\n",
    "    # mask creation\n",
    "    copy_img = img.read()\n",
    "    copy_img[copy_img < 0] = 0\n",
    "    copy_img = copy_img.swapaxes(0, 1)\n",
    "    copy_img = copy_img.swapaxes(1, 2)\n",
    "    mask = shadow_cloud_removal(copy_img)\n",
    "    mask[mask < 0] = 0\n",
    "\n",
    "    height = img_array.shape[0]\n",
    "    width = img_array.shape[1]\n",
    "    cols, rows = np.meshgrid(np.arange(width), np.arange(height))\n",
    "    xs, ys = rasterio.transform.xy(img.transform, rows, cols)\n",
    "    xs = np.array(xs)\n",
    "    ys = np.array(ys)\n",
    "\n",
    "    coordinates_list = get_coordinates_list(file)\n",
    "    df = pd.DataFrame(coordinates_list, columns=['index', 'lat', 'lon'])\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.lon, df.lat))\n",
    "    gdf.crs = \"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\"\n",
    "    gdf = gdf.to_crs('epsg:3976')\n",
    "\n",
    "    gdf['x'] = gdf.geometry.apply(lambda x: x.x)\n",
    "    gdf['y'] = gdf.geometry.apply(lambda x: x.y)\n",
    "\n",
    "    df = pd.DataFrame(gdf)\n",
    "    x_min = xs.min()\n",
    "    x_max = xs.max()\n",
    "    y_min = ys.min()\n",
    "    y_max = ys.max()\n",
    "    df = df[(df[\"x\"] >= x_min) & (df[\"x\"] <= x_max) & (df[\"y\"] >= y_min) & (df[\"y\"] <= y_max)]\n",
    "\n",
    "    # Map coordinates to pixel indices\n",
    "    pixel_coords = [img.index(x, y) for x, y in zip(df['x'], df['y'])]\n",
    "\n",
    "    # Dimensions for each patch\n",
    "    patch_size = 100\n",
    "    half_patch = patch_size // 2\n",
    "\n",
    "    # List to store the patches\n",
    "    patches = []\n",
    "    # List to store the mask patches\n",
    "    mask_patches = []\n",
    "    print(f\"Number of patches for {file}: {len(pixel_coords)}\")\n",
    "    for idx, (row_idx, col_idx) in enumerate(pixel_coords):  # Using enumerate to get the index\n",
    "        # Get the corresponding index from the DataFrame\n",
    "        index = df['index'].iloc[idx]\n",
    "\n",
    "        # Ensure the 32x32 patch falls within the original image boundaries\n",
    "        if (row_idx - half_patch >= 0 and row_idx + half_patch < height and \n",
    "            col_idx - half_patch >= 0 and col_idx + half_patch < width):\n",
    "\n",
    "            # Extract the 32x32 patch\n",
    "            patch = img_array[row_idx - half_patch:row_idx + half_patch, \n",
    "                              col_idx - half_patch:col_idx + half_patch]\n",
    "            patches.append(patch)\n",
    "\n",
    "            # Extract the mask patch\n",
    "            mask_patch = mask[row_idx - half_patch:row_idx + half_patch, \n",
    "                              col_idx - half_patch:col_idx + half_patch]\n",
    "            mask_patches.append(mask_patch)\n",
    "\n",
    "            # Save the patch\n",
    "            patch_filename = os.path.join(path1, f'grid_images/train/patch_{index}_{row_idx}_{col_idx}.png')\n",
    "            plt.imsave(patch_filename, patch)\n",
    "            # Save the mask patch\n",
    "            mask_patch_filename = os.path.join(path1, f'grid_images/mask/mask_patch_{index}_{row_idx}_{col_idx}.png')\n",
    "            plt.imsave(mask_patch_filename, mask_patch)\n",
    "\n",
    "    return patches\n",
    "\n",
    "# for file in tiff_files:\n",
    "#     if os.path.basename(file) in [\"s2_vis_41_20191120T200529_20191120T200529_T02CME.tif\", \"s2_vis_46_20191123T183459_20191123T183459_T04CEA.tif\"]:\n",
    "#         print(f\"Skipping {file}\")\n",
    "        # create_patches_from_coordinates(file)\n",
    "    # print(f\"Number of patches for {file}: {len(res[0])}\")\n",
    "    # print(f\"Number of mask patches for {file}: {len(res[1])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "# matplotlib.use('Agg')  # Use non-interactive backend\n",
    "# matplotlib.use('TkAgg')\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "def plot_grid_and_coordinates(file):\n",
    "    # Open image\n",
    "    img = rasterio.open(file, crs='EPSG:3976')\n",
    "\n",
    "    # Read the original image without any processing\n",
    "    original_img = img.read()\n",
    "    # Swap axes to get correct orientation (bands last)\n",
    "    original_img = original_img.swapaxes(0, 2)\n",
    "    original_img = original_img.swapaxes(0, 1)\n",
    "\n",
    "    # Get dimensions from original image\n",
    "    height = original_img.shape[0]\n",
    "    width = original_img.shape[1]\n",
    "\n",
    "    # Create coordinate grid\n",
    "    cols, rows = np.meshgrid(np.arange(width), np.arange(height))\n",
    "    xs, ys = rasterio.transform.xy(img.transform, rows, cols)\n",
    "    xs = np.array(xs)\n",
    "    ys = np.array(ys)\n",
    "\n",
    "    # Convert coordinates to GeoDataFrame\n",
    "    coordinates_list = get_coordinates_list(file)\n",
    "    df = pd.DataFrame(coordinates_list, columns=['index', 'lat', 'lon'])\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.lon, df.lat))\n",
    "    gdf.crs = \"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\"\n",
    "\n",
    "    # Transform to match the image's CRS\n",
    "    gdf = gdf.to_crs('epsg:3976')\n",
    "\n",
    "    # Extract transformed coordinates\n",
    "    gdf['x'] = gdf.geometry.apply(lambda x: x.x)\n",
    "    gdf['y'] = gdf.geometry.apply(lambda x: x.y)\n",
    "\n",
    "    # Filter coordinates\n",
    "    df = pd.DataFrame(gdf)\n",
    "    x_min = xs.min()\n",
    "    x_max = xs.max()\n",
    "    y_min = ys.min()\n",
    "    y_max = ys.max()\n",
    "    df = df[(df[\"x\"] >= x_min) & (df[\"x\"] <= x_max) & \n",
    "            (df[\"y\"] >= y_min) & (df[\"y\"] <= y_max)]\n",
    "\n",
    "    # Map coordinates to pixel indices\n",
    "    pixel_coords = [img.index(x, y) for x, y in zip(df['x'], df['y'])]\n",
    "\n",
    "    # Generate grid lines\n",
    "    x_coords = np.linspace(0, width, 33)\n",
    "    y_coords = np.linspace(0, height, 33)\n",
    "\n",
    "    # Plot\n",
    "    plt.imshow(original_img)  # Plot original image without any processing\n",
    "    plt.vlines(x_coords, 0, height, color='red', linewidth=0.5)\n",
    "    plt.hlines(y_coords, 0, width, color='red', linewidth=0.5)\n",
    "    \n",
    "    rows, cols = zip(*pixel_coords)\n",
    "    plt.scatter(cols, rows, color='blue', s=2)\n",
    "    \n",
    "    plt.title(f'Grid Lines and Coordinates for {os.path.basename(file)}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot grid lines and coordinates for the first two files\n",
    "# for file in tiff_files:\n",
    "#     if os.path.basename(file) in [\"s2_vis_46_20191123T183459_20191123T183459_T04CEA.tif\"]:\n",
    "#         plot_grid_and_coordinates(file)\n",
    "    # if os.path.basename(file) in [\"s2_vis_41_20191120T200529_20191120T200529_T02CME.tif\", \"s2_vis_27_20191116T184459_20191116T184458_T02CMU.tif\"]:\n",
    "    #     plot_grid_and_coordinates(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch and generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read ATLO3 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "output_file = os.path.join(path1, \"clean\", \"cleaned_data_indexed.csv\")\n",
    "\n",
    "# separate features and labels\n",
    "dataset_accu = pd.read_csv(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class distribution plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIMIT_BATCH_SIZE = 2900\n",
    "# sampler_ds = dataset_accu.copy()\n",
    "\n",
    "# # Get the indices of each class\n",
    "# class_0_indices = sampler_ds[sampler_ds['label'] == 0].index\n",
    "# class_1_indices = sampler_ds[sampler_ds['label'] == 1].index\n",
    "# class_2_indices = sampler_ds[sampler_ds['label'] == 2].index\n",
    "\n",
    "# # Get the number of samples for each class\n",
    "# num_samples_class_0 = int(LIMIT_BATCH_SIZE * 0.333333)\n",
    "# num_samples_class_1 = int(LIMIT_BATCH_SIZE * 0.3)\n",
    "# num_samples_class_2 = int(LIMIT_BATCH_SIZE * 0.3)\n",
    "\n",
    "# # Get the random samples for each class\n",
    "# random_class_0_indices = np.random.choice(class_0_indices, num_samples_class_0, replace=False)\n",
    "# random_class_1_indices = np.random.choice(class_1_indices, num_samples_class_1, replace=False)\n",
    "# random_class_2_indices = np.random.choice(class_2_indices, num_samples_class_2, replace=False)\n",
    "\n",
    "# # Concatenate the random samples\n",
    "# random_indices = np.concatenate([random_class_0_indices, random_class_1_indices, random_class_2_indices])\n",
    "\n",
    "# # Get the sampled dataset\n",
    "# sampler_ds = sampler_ds.loc[random_indices]\n",
    "# images_path = os.path.join(path1, \"grid_images/train/\", \"*.png\")\n",
    "# images_mask_path = os.path.join(path1, \"grid_images/mask\")\n",
    "\n",
    "# # Get list of image files and mask files for the sampled dataset that matches the indices\n",
    "# image_files = sorted(glob.glob(images_path))\n",
    "# mask_files = sorted(glob.glob(images_mask_path + \"/*.png\"))\n",
    "\n",
    "# available_indices = set()\n",
    "# train_images_ = []\n",
    "# train_masks_ = []\n",
    "# for filename in sorted(glob.glob(images_path)):\n",
    "#     temp = os.path.basename(filename).split(\"_\")\n",
    "#     index = int(temp[1])\n",
    "#     if index not in random_indices:\n",
    "#         continue\n",
    "#     img = cv2.imread(filename, 0)\n",
    "#     if img is None:\n",
    "#         print(f\"Warning: Could not load image {filename}. Skipping.\")\n",
    "#         continue\n",
    "#     temp = temp[1:]\n",
    "#     temp = \"mask_patch_\" + \"_\".join(temp)\n",
    "#     temp = os.path.join(images_mask_path, temp)\n",
    "    \n",
    "#     if not os.path.exists(temp):\n",
    "#         continue\n",
    "#     ####################### ####################### ####################### ####################### #######################\n",
    "#     if index in available_indices: # VERY CRUCIAL   ####################### ####################### #######################\n",
    "#         continue\n",
    "#     available_indices.add(index)\n",
    "#     train_images_.append(filename)\n",
    "#     train_masks_.append(temp)\n",
    "\n",
    "# # print size of the train_images and train_masks\n",
    "# print(len(train_images_))\n",
    "# print(len(train_masks_))\n",
    "\n",
    "# # from available indices, get the corresponding labels\n",
    "# labels = []\n",
    "# for index in available_indices:\n",
    "#     labels.append(sampler_ds[sampler_ds['index'] == index]['label'].values[0])\n",
    "\n",
    "# # show plot of the labels distribution\n",
    "# plt.hist(labels, bins=3)\n",
    "# # count of each class\n",
    "# print(np.unique(labels, return_counts=True))\n",
    "\n",
    "# print(f\"Number of available indices: {len(available_indices)}\")\n",
    "# print(f\"Number of train images: {len(train_images_)}\")\n",
    "# print(f\"Number of train masks: {len(train_masks_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image filtering based on ATLO3 files indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available indices: 3267\n",
      "Number of train images: 3267\n",
      "Number of train masks: 3267\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# available indices set\n",
    "available_indices = set()\n",
    "train_images_ = []\n",
    "train_masks_ = []\n",
    "images_path = os.path.join(path1, \"grid_images/train/\", \"*.png\")\n",
    "images_mask_path = os.path.join(path1, \"grid_images/mask\")\n",
    "for filename in sorted(glob.glob(images_path))[:LIMIT_BATCH_SIZE]:\n",
    "    img = cv2.imread(filename, 0)\n",
    "    if img is None:\n",
    "        print(f\"Warning: Could not load image {filename}. Skipping.\")\n",
    "        continue\n",
    "    temp = os.path.basename(filename).split(\"_\")\n",
    "    index = int(temp[1])\n",
    "    temp = temp[1:]\n",
    "    temp = \"mask_patch_\" + \"_\".join(temp)\n",
    "    temp = os.path.join(images_mask_path, temp)\n",
    "    \n",
    "    if not os.path.exists(temp):\n",
    "        continue\n",
    "    ####################### ####################### ####################### ####################### #######################\n",
    "    if index in available_indices: # VERY CRUCIAL   ####################### ####################### #######################\n",
    "        continue\n",
    "    available_indices.add(index)\n",
    "    train_images_.append(filename)\n",
    "    train_masks_.append(temp)\n",
    "    ####################### ####################### ####################### ####################### #######################\n",
    "\n",
    "\n",
    "print(f\"Number of available indices: {len(available_indices)}\")\n",
    "print(f\"Number of train images: {len(train_images_)}\")\n",
    "print(f\"Number of train masks: {len(train_masks_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Uni-modals here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, clone_model\n",
    "from tensorflow.keras.layers import Flatten, Multiply, Dense, Input, Reshape, Lambda\n",
    "\n",
    "\n",
    "# Load the pre-trained UNet model\n",
    "unet_model = tf.keras.models.load_model('s2_multi_with_cloud_auto_labeled_50.keras')\n",
    "lstm_model = get_lstm_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_features.shape (3267,)\n",
      "(3267, 21)\n",
      "(3267, 5, 4)\n",
      "The train set size is 1959, validation set size is 654, and testing set size is 654\n",
      "The shapes of training data are (1959, 5, 4) (1959, 3)\n",
      "The shapes of testing data are (654, 5, 4) (654, 3)\n",
      "The shapes of testing data are (654, 5, 4) (654, 3)\n"
     ]
    }
   ],
   "source": [
    "lstm_features = dataset_accu.copy()\n",
    "# remove rows with index not in available_indices\n",
    "lstm_features = lstm_features[lstm_features['index'].isin(available_indices)]\n",
    "lstm_labels = lstm_features.pop('label')\n",
    "lstm_track = lstm_features.pop('date')\n",
    "lstm_track = lstm_features.pop('lat')\n",
    "lstm_track = lstm_features.pop('lon')\n",
    "print(\"lstm_features.shape\", lstm_labels.shape)\n",
    "\n",
    "\n",
    "# normalized the features\n",
    "def norm(x, M = 1, m = 0):\n",
    "    # Normalize the input data\n",
    "    # output = (x-m)/(M-m) # Max-min normalization\n",
    "    output = (x-x.mean())/(M-x.std()) # mean-std normalization\n",
    "    return output\n",
    "\n",
    "norm_lstm_features = norm(lstm_features)\n",
    "norm_lstm_features\n",
    "\n",
    "\n",
    "# convert the data to LSTM format\n",
    "np_array = norm_lstm_features.to_numpy()\n",
    "print(np_array.shape)\n",
    "new_lstm_features = []\n",
    "for row in np_array:\n",
    "  # construct features array\n",
    "  point_r2 =  row[1:5]\n",
    "  point_r1 =  row[5:9]\n",
    "  point_0 = row[9:13]\n",
    "  point_l1 = row[13:17]\n",
    "  point_l2 = row[17:21]\n",
    "  new_lstm_features.append([point_r2, point_r1, point_0, point_l1, point_l2])\n",
    "\n",
    "new_lstm_features = np.array(new_lstm_features)\n",
    "print(new_lstm_features.shape) # the X array inputshape = [5,4]\n",
    "\n",
    "\n",
    "\n",
    "# covert labels to multi-class\n",
    "def convert_to_multi_calss(data):\n",
    "  new_labels = []\n",
    "  for label in data:\n",
    "    #print(row)\n",
    "    if label == 0:\n",
    "      new_labels.append([1,0,0]) \n",
    "    elif label == 1:\n",
    "      new_labels.append([0,1,0])\n",
    "    elif label == 2:\n",
    "      new_labels.append([0,0,1])\n",
    "    else:\n",
    "      print(\"Error: invalid label:\" + label)\n",
    "      return \n",
    "  return np.array(new_labels)\n",
    "\n",
    "lstm_labels_multi = convert_to_multi_calss(lstm_labels)\n",
    "lstm_labels_multi\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(new_lstm_features, lstm_labels_multi, \n",
    "                                                    test_size =0.2, shuffle=True, random_state=20)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size =0.25, shuffle=True, random_state=20)\n",
    "\n",
    "print(\"The train set size is {0:d}, validation set size is {1:d}, \"\n",
    "      \"and testing set size is {2:d}\".format(len(X_train), len(X_val), len(X_test)))\n",
    "print(\"The shapes of training data are\", X_train.shape, Y_train.shape)\n",
    "print(\"The shapes of testing data are\", X_test.shape, Y_test.shape)\n",
    "print(\"The shapes of testing data are\", X_val.shape, Y_val.shape)\n",
    "\n",
    "\n",
    "#loss_func = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "loss_func = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "############################################################################################################\n",
    "###########                      LSTM MODEL TRAINING                                         ###############\n",
    "############################################################################################################\n",
    "\"\"\"\n",
    "\n",
    "model = get_lstm_model()\n",
    "# plot_model(\n",
    "#     model, \n",
    "#     to_file='lst_model.png', \n",
    "#     show_shapes=True, \n",
    "#     show_layer_names=False, \n",
    "#     rankdir='TB', \n",
    "#     expand_nested=False, \n",
    "#     dpi=96\n",
    "# )\n",
    "# history = train_model(model, X_train, Y_train, X_val, Y_val, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "# plot_training_history(history)\n",
    "# test_loss, test_accuracy = evaluate_model(model, X_test, Y_test)\n",
    "\n",
    "# print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "# print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing images for UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] Save the UNet model in keras format\n",
    "- [x] Load the LSTM model in a variable\n",
    "- [x] Perform additive fusion\n",
    "- [x] Perform multiplication fusion\n",
    "- [x] Perform Gated fusion\n",
    "- [ ] Perform Nonlinear fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214106112 214106112\n",
      "(3267, 256, 256) (3267, 256, 256)\n",
      "train image shape (3267, 256, 256)\n",
      "train masks shape (3267, 256, 256)\n",
      "\n",
      "train image shape (3267, 256, 256, 1)\n",
      "train masks input shape (3267, 256, 256, 1)\n",
      "Class values in the dataset are ...  [0 1 2]\n",
      "X_train_unet train image shape (2613, 256, 256, 1)\n",
      "X_test_unet train image shape (654, 256, 256, 1)\n",
      "y_train_cat shape (2613, 256, 256, 3)\n",
      "y_test_cat shape (654, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "# Preparing the data for the fusion model\n",
    "\n",
    "#Resizing images, if needed\n",
    "SIZE_X = 256 #128 \n",
    "SIZE_Y = 256 #128\n",
    "n_classes = 3 #Number of classes for segmentation\n",
    "path1 = os.path.abspath('../')\n",
    "path_training_100x100_image = os.path.join(path1, \"grid_images/train\")\n",
    "path_mask_100x100_image = os.path.join(path1, \"grid_images/mask\")\n",
    "#training image as a list\n",
    "train_images_list = train_images_\n",
    "train_images = []\n",
    "\n",
    "\n",
    "for directory_path in glob.glob(path_training_100x100_image):\n",
    "    for img_path in sorted(glob.glob(os.path.join(directory_path, \"*.png\"))):\n",
    "        if img_path not in train_images_list:\n",
    "            continue\n",
    "        img = cv2.imread(img_path, 0)\n",
    "        if img is None:\n",
    "            print(f\"Warning: Could not load image {img_path}. Skipping.\")\n",
    "            continue\n",
    "        img = cv2.resize(img, (SIZE_Y, SIZE_X))\n",
    "        train_images.append(img)\n",
    "\n",
    "#Convert list to array for machine learning processing\n",
    "train_images = np.array(train_images)\n",
    "\n",
    "#training mask/label as a list\n",
    "mask_images_list = train_masks_\n",
    "train_masks = [] \n",
    "\n",
    "for directory_path in glob.glob(path_mask_100x100_image):\n",
    "    for mask_path in sorted(glob.glob(os.path.join(directory_path, \"*.png\"))):\n",
    "        if mask_path not in mask_images_list:\n",
    "            continue\n",
    "        mask = cv2.imread(mask_path, 0)\n",
    "        mask = cv2.resize(mask, (SIZE_Y, SIZE_X), interpolation = cv2.INTER_NEAREST)  #Otherwise ground truth changes due to interpolation\n",
    "        train_masks.append(mask)\n",
    "#Convert list to array for machine learning processing\n",
    "\n",
    "train_masks = np.array(train_masks)\n",
    "print(np.size(train_masks), np.size(train_images))\n",
    "np.unique(train_masks)\n",
    "print(train_images.shape, train_masks.shape)\n",
    "# plot_image(train_images[1], train_masks[1])\n",
    "\n",
    "###############################################\n",
    "#Encode labels... but multi dim array so need to flatten, encode and reshape\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "n, h, w = train_masks.shape\n",
    "train_masks_reshaped = train_masks.reshape(-1,1)\n",
    "train_masks_reshaped_encoded = labelencoder.fit_transform(train_masks_reshaped)\n",
    "train_masks_encoded_original_shape = train_masks_reshaped_encoded.reshape(n, h, w)\n",
    "\n",
    "np.unique(train_masks_encoded_original_shape)\n",
    "\n",
    "print(\"train image shape\", train_images.shape)\n",
    "print(\"train masks shape\", train_masks.shape)\n",
    "\n",
    "#################################################\n",
    "train_images = np.expand_dims(train_images, axis=3)\n",
    "train_images = normalize(train_images, axis=1)\n",
    "\n",
    "train_masks_input = np.expand_dims(train_masks_encoded_original_shape, axis=3)\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"train image shape\", train_images.shape)\n",
    "print(\"train masks input shape\", train_masks_input.shape)\n",
    "\n",
    "#################################################\n",
    "# Here I take only train_masks_input and comment out the rest of the code\n",
    "#################################################\n",
    "#Selecting 20% for testing and remaining for training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_unet, X_test_unet, y_train_unet, y_test_unet = train_test_split(train_images, train_masks_input, test_size=0.20, random_state=0)\n",
    "\n",
    "print(\"Class values in the dataset are ... \", np.unique(y_train_unet))\n",
    "\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "train_masks_cat = to_categorical(y_train_unet, num_classes=n_classes)\n",
    "y_train_cat = train_masks_cat.reshape((y_train_unet.shape[0], y_train_unet.shape[1], y_train_unet.shape[2], n_classes))\n",
    "\n",
    "\n",
    "\n",
    "test_masks_cat = to_categorical(y_test_unet, num_classes=n_classes)\n",
    "y_test_cat = test_masks_cat.reshape((y_test_unet.shape[0], y_test_unet.shape[1], y_test_unet.shape[2], n_classes))\n",
    "\n",
    "\n",
    "print(\"X_train_unet train image shape\", X_train_unet.shape)\n",
    "print(\"X_test_unet train image shape\", X_test_unet.shape)\n",
    "print(\"y_train_cat shape\", y_train_cat.shape)\n",
    "print(\"y_test_cat shape\", y_test_cat.shape)\n",
    "\n",
    "# IMG_HEIGHT = X_train.shape[1]\n",
    "# IMG_WIDTH  = X_train.shape[2]\n",
    "# IMG_CHANNELS = X_train.shape[3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNet model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get model\n",
    "# history = train_model(\n",
    "#     unet_model,\n",
    "#     X_train_unet,\n",
    "#     y_train_cat,\n",
    "#     X_test_unet,\n",
    "#     y_test_cat,\n",
    "#     epochs=EPOCHS,\n",
    "#     batch_size=BATCH_SIZE\n",
    "# )\n",
    "\n",
    "# # Evaluate the model\n",
    "# test_loss, test_accuracy = evaluate_model(unet_model, X_test_unet, y_test_cat)\n",
    "# print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "# print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# # Plot training history\n",
    "# plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning on ATLO3 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h_cor_mean-2</th>\n",
       "      <th>height_sd-2</th>\n",
       "      <th>pcnth_mean-2</th>\n",
       "      <th>pcnt_mean-2</th>\n",
       "      <th>h_cor_mean-1</th>\n",
       "      <th>height_sd-1</th>\n",
       "      <th>pcnth_mean-1</th>\n",
       "      <th>pcnt_mean-1</th>\n",
       "      <th>h_cor_mean0</th>\n",
       "      <th>height_sd0</th>\n",
       "      <th>pcnth_mean0</th>\n",
       "      <th>pcnt_mean0</th>\n",
       "      <th>h_cor_mean1</th>\n",
       "      <th>height_sd1</th>\n",
       "      <th>pcnth_mean1</th>\n",
       "      <th>pcnt_mean1</th>\n",
       "      <th>h_cor_mean2</th>\n",
       "      <th>height_sd2</th>\n",
       "      <th>pcnth_mean2</th>\n",
       "      <th>pcnt_mean2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>0.791500</td>\n",
       "      <td>0.169983</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>0.971037</td>\n",
       "      <td>0.070724</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>0.888939</td>\n",
       "      <td>0.173739</td>\n",
       "      <td>2.714286</td>\n",
       "      <td>8.571429</td>\n",
       "      <td>0.842807</td>\n",
       "      <td>0.186369</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.922513</td>\n",
       "      <td>0.082469</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>6.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>0.971037</td>\n",
       "      <td>0.070724</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>0.888939</td>\n",
       "      <td>0.173739</td>\n",
       "      <td>2.714286</td>\n",
       "      <td>8.571429</td>\n",
       "      <td>0.842807</td>\n",
       "      <td>0.186369</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.922513</td>\n",
       "      <td>0.082469</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>0.796576</td>\n",
       "      <td>0.165304</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>6.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>0.888939</td>\n",
       "      <td>0.173739</td>\n",
       "      <td>2.714286</td>\n",
       "      <td>8.571429</td>\n",
       "      <td>0.842807</td>\n",
       "      <td>0.186369</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.922513</td>\n",
       "      <td>0.082469</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>0.796576</td>\n",
       "      <td>0.165304</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>6.916667</td>\n",
       "      <td>0.754750</td>\n",
       "      <td>0.127537</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>7.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>0.842807</td>\n",
       "      <td>0.186369</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.922513</td>\n",
       "      <td>0.082469</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>0.796576</td>\n",
       "      <td>0.165304</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>6.916667</td>\n",
       "      <td>0.754750</td>\n",
       "      <td>0.127537</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>0.846794</td>\n",
       "      <td>0.108723</td>\n",
       "      <td>5.888889</td>\n",
       "      <td>7.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <td>0.922513</td>\n",
       "      <td>0.082469</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>0.796576</td>\n",
       "      <td>0.165304</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>6.916667</td>\n",
       "      <td>0.754750</td>\n",
       "      <td>0.127537</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>0.846794</td>\n",
       "      <td>0.108723</td>\n",
       "      <td>5.888889</td>\n",
       "      <td>7.888889</td>\n",
       "      <td>0.774610</td>\n",
       "      <td>0.110507</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>5.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102965</th>\n",
       "      <td>0.663228</td>\n",
       "      <td>0.700268</td>\n",
       "      <td>5.400000</td>\n",
       "      <td>9.066667</td>\n",
       "      <td>0.511376</td>\n",
       "      <td>0.569917</td>\n",
       "      <td>7.782609</td>\n",
       "      <td>11.304348</td>\n",
       "      <td>0.413798</td>\n",
       "      <td>0.347209</td>\n",
       "      <td>8.680000</td>\n",
       "      <td>12.040000</td>\n",
       "      <td>0.391911</td>\n",
       "      <td>0.271459</td>\n",
       "      <td>5.941176</td>\n",
       "      <td>7.176471</td>\n",
       "      <td>0.490262</td>\n",
       "      <td>0.221304</td>\n",
       "      <td>7.533333</td>\n",
       "      <td>11.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102966</th>\n",
       "      <td>0.511376</td>\n",
       "      <td>0.569917</td>\n",
       "      <td>7.782609</td>\n",
       "      <td>11.304348</td>\n",
       "      <td>0.413798</td>\n",
       "      <td>0.347209</td>\n",
       "      <td>8.680000</td>\n",
       "      <td>12.040000</td>\n",
       "      <td>0.391911</td>\n",
       "      <td>0.271459</td>\n",
       "      <td>5.941176</td>\n",
       "      <td>7.176471</td>\n",
       "      <td>0.490262</td>\n",
       "      <td>0.221304</td>\n",
       "      <td>7.533333</td>\n",
       "      <td>11.466667</td>\n",
       "      <td>0.528215</td>\n",
       "      <td>0.321206</td>\n",
       "      <td>9.888889</td>\n",
       "      <td>14.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102967</th>\n",
       "      <td>0.413798</td>\n",
       "      <td>0.347209</td>\n",
       "      <td>8.680000</td>\n",
       "      <td>12.040000</td>\n",
       "      <td>0.391911</td>\n",
       "      <td>0.271459</td>\n",
       "      <td>5.941176</td>\n",
       "      <td>7.176471</td>\n",
       "      <td>0.490262</td>\n",
       "      <td>0.221304</td>\n",
       "      <td>7.533333</td>\n",
       "      <td>11.466667</td>\n",
       "      <td>0.528215</td>\n",
       "      <td>0.321206</td>\n",
       "      <td>9.888889</td>\n",
       "      <td>14.037037</td>\n",
       "      <td>0.442396</td>\n",
       "      <td>0.284981</td>\n",
       "      <td>7.363636</td>\n",
       "      <td>8.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102968</th>\n",
       "      <td>0.391911</td>\n",
       "      <td>0.271459</td>\n",
       "      <td>5.941176</td>\n",
       "      <td>7.176471</td>\n",
       "      <td>0.490262</td>\n",
       "      <td>0.221304</td>\n",
       "      <td>7.533333</td>\n",
       "      <td>11.466667</td>\n",
       "      <td>0.528215</td>\n",
       "      <td>0.321206</td>\n",
       "      <td>9.888889</td>\n",
       "      <td>14.037037</td>\n",
       "      <td>0.442396</td>\n",
       "      <td>0.284981</td>\n",
       "      <td>7.363636</td>\n",
       "      <td>8.045455</td>\n",
       "      <td>0.473765</td>\n",
       "      <td>0.144270</td>\n",
       "      <td>7.782609</td>\n",
       "      <td>9.869565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102969</th>\n",
       "      <td>0.490262</td>\n",
       "      <td>0.221304</td>\n",
       "      <td>7.533333</td>\n",
       "      <td>11.466667</td>\n",
       "      <td>0.528215</td>\n",
       "      <td>0.321206</td>\n",
       "      <td>9.888889</td>\n",
       "      <td>14.037037</td>\n",
       "      <td>0.442396</td>\n",
       "      <td>0.284981</td>\n",
       "      <td>7.363636</td>\n",
       "      <td>8.045455</td>\n",
       "      <td>0.473765</td>\n",
       "      <td>0.144270</td>\n",
       "      <td>7.782609</td>\n",
       "      <td>9.869565</td>\n",
       "      <td>0.355467</td>\n",
       "      <td>0.145530</td>\n",
       "      <td>9.357143</td>\n",
       "      <td>12.071429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3267 rows  20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        h_cor_mean-2  height_sd-2  pcnth_mean-2  pcnt_mean-2  h_cor_mean-1  \\\n",
       "10000       0.791500     0.169983      1.000000     6.500000      0.971037   \n",
       "10001       0.971037     0.070724      3.600000     7.400000      0.888939   \n",
       "10002       0.888939     0.173739      2.714286     8.571429      0.842807   \n",
       "10003       0.842807     0.186369      2.000000     7.000000      0.922513   \n",
       "10004       0.922513     0.082469      1.800000     6.400000      0.796576   \n",
       "...              ...          ...           ...          ...           ...   \n",
       "102965      0.663228     0.700268      5.400000     9.066667      0.511376   \n",
       "102966      0.511376     0.569917      7.782609    11.304348      0.413798   \n",
       "102967      0.413798     0.347209      8.680000    12.040000      0.391911   \n",
       "102968      0.391911     0.271459      5.941176     7.176471      0.490262   \n",
       "102969      0.490262     0.221304      7.533333    11.466667      0.528215   \n",
       "\n",
       "        height_sd-1  pcnth_mean-1  pcnt_mean-1  h_cor_mean0  height_sd0  \\\n",
       "10000      0.070724      3.600000     7.400000     0.888939    0.173739   \n",
       "10001      0.173739      2.714286     8.571429     0.842807    0.186369   \n",
       "10002      0.186369      2.000000     7.000000     0.922513    0.082469   \n",
       "10003      0.082469      1.800000     6.400000     0.796576    0.165304   \n",
       "10004      0.165304      4.166667     6.916667     0.754750    0.127537   \n",
       "...             ...           ...          ...          ...         ...   \n",
       "102965     0.569917      7.782609    11.304348     0.413798    0.347209   \n",
       "102966     0.347209      8.680000    12.040000     0.391911    0.271459   \n",
       "102967     0.271459      5.941176     7.176471     0.490262    0.221304   \n",
       "102968     0.221304      7.533333    11.466667     0.528215    0.321206   \n",
       "102969     0.321206      9.888889    14.037037     0.442396    0.284981   \n",
       "\n",
       "        pcnth_mean0  pcnt_mean0  h_cor_mean1  height_sd1  pcnth_mean1  \\\n",
       "10000      2.714286    8.571429     0.842807    0.186369     2.000000   \n",
       "10001      2.000000    7.000000     0.922513    0.082469     1.800000   \n",
       "10002      1.800000    6.400000     0.796576    0.165304     4.166667   \n",
       "10003      4.166667    6.916667     0.754750    0.127537     4.333333   \n",
       "10004      4.333333    7.333333     0.846794    0.108723     5.888889   \n",
       "...             ...         ...          ...         ...          ...   \n",
       "102965     8.680000   12.040000     0.391911    0.271459     5.941176   \n",
       "102966     5.941176    7.176471     0.490262    0.221304     7.533333   \n",
       "102967     7.533333   11.466667     0.528215    0.321206     9.888889   \n",
       "102968     9.888889   14.037037     0.442396    0.284981     7.363636   \n",
       "102969     7.363636    8.045455     0.473765    0.144270     7.782609   \n",
       "\n",
       "        pcnt_mean1  h_cor_mean2  height_sd2  pcnth_mean2  pcnt_mean2  \n",
       "10000     7.000000     0.922513    0.082469     1.800000    6.400000  \n",
       "10001     6.400000     0.796576    0.165304     4.166667    6.916667  \n",
       "10002     6.916667     0.754750    0.127537     4.333333    7.333333  \n",
       "10003     7.333333     0.846794    0.108723     5.888889    7.888889  \n",
       "10004     7.888889     0.774610    0.110507     2.333333    5.166667  \n",
       "...            ...          ...         ...          ...         ...  \n",
       "102965    7.176471     0.490262    0.221304     7.533333   11.466667  \n",
       "102966   11.466667     0.528215    0.321206     9.888889   14.037037  \n",
       "102967   14.037037     0.442396    0.284981     7.363636    8.045455  \n",
       "102968    8.045455     0.473765    0.144270     7.782609    9.869565  \n",
       "102969    9.869565     0.355467    0.145530     9.357143   12.071429  \n",
       "\n",
       "[3267 rows x 20 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(3267, 5, 4)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete all rows in lstm data that are not in available_indices\n",
    "lstm_dataset = dataset_accu.copy()\n",
    "# size before\n",
    "# print(lstm_dataset.shape)\n",
    "lstm_features_synced = lstm_dataset[lstm_dataset['index'].isin(available_indices)]\n",
    "\n",
    "# separate features and labels\n",
    "lstm_labels = lstm_features_synced.pop('label')\n",
    "lstm_features_synced.pop('date')\n",
    "lstm_features_synced.pop('index')\n",
    "lstm_features_synced.pop('lat')\n",
    "lstm_features_synced.pop('lon')\n",
    "display(lstm_features_synced)\n",
    "\n",
    "# normalized the features\n",
    "norm_lstm_features = norm(lstm_features_synced)\n",
    "\n",
    "new_lstm_features = []\n",
    "for row in np_array:\n",
    "  # construct features array\n",
    "  point_r2 =  row[1:5]\n",
    "  point_r1 =  row[5:9]\n",
    "  point_0 = row[9:13]\n",
    "  point_l1 = row[13:17]\n",
    "  point_l2 = row[17:21]\n",
    "  new_lstm_features.append([point_r2, point_r1, point_0, point_l1, point_l2])\n",
    "new_lstm_features = np.array(new_lstm_features)\n",
    "#################################################\n",
    "lstm_labels_multi = convert_to_multi_calss(lstm_labels)\n",
    "new_lstm_features.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_6\n",
      "dropout_75\n",
      "dense_94\n",
      "dropout_76\n",
      "dense_95\n",
      "dropout_77\n",
      "dense_96\n",
      "dropout_78\n",
      "dense_97\n",
      "dropout_79\n",
      "dense_98\n",
      "dropout_80\n",
      "dense_99\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Clone the models with a unique prefix to avoid layer name conflicts\n",
    "unet_model_cloned = clone_model(unet_model)\n",
    "unet_model_cloned.set_weights(unet_model.get_weights())\n",
    "lstm_model_cloned = clone_model(lstm_model)\n",
    "lstm_model_cloned.set_weights(lstm_model.get_weights())\n",
    "\n",
    "# Rename layers to avoid conflicts\n",
    "for layer in unet_model_cloned.layers:\n",
    "    layer.name = \"unet_\" + layer.name\n",
    "\n",
    "for layer in lstm_model_cloned.layers:\n",
    "    print(layer.name)\n",
    "    layer.name = \"lstm_\" + layer.name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the fusion models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get input and output layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unet output <KerasTensor shape=(None, 256, 256, 16), dtype=float32, sparse=False, name=keras_tensor_1579>\n",
      "lstm output (None, 16)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "To creates the fusion model we need to understand the shapes of our inputs:\n",
    "- Input of the LSTM model is a 5x4 matrix & is found on the first index of the models layers list.\n",
    "- Input of the UNet model is a 256x256x1 image (Resized from 100x100)\n",
    "\n",
    "- To build the fusion, we will extract the features from the last hidden layer of both models.\n",
    "- We will then flatten the features from the UNet model to match the shape of the LSTM model.\n",
    "\"\"\"\n",
    "\n",
    "# Get the input layers\n",
    "unet_input = unet_model_cloned.input\n",
    "lstm_input = lstm_model_cloned.layers[0].input\n",
    "# Extract features (fusion layers) from the cloned UNet and LSTM models\n",
    "unet_features = unet_model_cloned.layers[-2].output  # Last layer before the output\n",
    "lstm_features = lstm_model_cloned.layers[-2].output  # Last layer before the output\n",
    "\n",
    "# print the shapes of the features\n",
    "print(\"unet output\", unet_features)\n",
    "print(\"lstm output\", lstm_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Center extraction for the UNet and Reshaping of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before flattening unet output (None, 256, 256, 16)\n",
      "after flattening unet output (None, 16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_center_pixel(x):\n",
    "    ceter_h = x.shape[1] // 2\n",
    "    ceter_w = x.shape[2] // 2\n",
    "    return x[:, ceter_h, ceter_w, :]\n",
    "\n",
    "# Flatten the vectors and ready the layers in the middle\n",
    "unet_center = Lambda(extract_center_pixel)(unet_features)  # Shape: (None, 1, 1, 16)\n",
    "unet_features_flat = Reshape((16,))(unet_center)  # Shape: (None, 16)\n",
    "print(\"before flattening unet output\", unet_features.shape)\n",
    "print(\"after flattening unet output\", unet_features_flat.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EYE mtrx for missing modality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_missing_with_identity(input_data, expected_shape):\n",
    "    \"\"\"\n",
    "    Replace missing input data (None) with an identity matrix of the appropriate shape.\n",
    "    \"\"\"\n",
    "    if input_data is None:\n",
    "        # Create an identity matrix for the feature dimensions\n",
    "        batch_size = expected_shape[0]  # Use batch size from input if available\n",
    "        features_dim = expected_shape[-1]\n",
    "        # Create an identity matrix and tile to match the batch size\n",
    "        identity_matrix = np.eye(features_dim)\n",
    "        identity_matrix = np.tile(identity_matrix, (batch_size, 1))\n",
    "        return identity_matrix  # Return as a NumPy array\n",
    "    return input_data  # Ensure this is a NumPy array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero padding for missing modality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import ZeroPadding1D\n",
    "\n",
    "# If input is None, use zeros for padding\n",
    "def replace_missing_with_zeros(input_tensor, expected_shape):\n",
    "    if input_tensor is None:\n",
    "        return tf.zeros(shape=expected_shape, dtype=tf.float32)\n",
    "    return input_tensor\n",
    "\n",
    "# Replace missing input with zeros instead\n",
    "unet_input = replace_missing_with_zeros(unet_input, expected_shape=(BATCH_SIZE, unet_input.shape[-1]))\n",
    "lstm_input = replace_missing_with_zeros(lstm_input, expected_shape=(BATCH_SIZE, lstm_input.shape[-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for multiplication fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's output shape (None, 3)\n",
      "Expected input shape:  [(None, 256, 256, 1), (None, 5, 4)]\n",
      "Expected output shape:  (None, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "############################################################################################################\n",
    "###########                      MULTIPLICATIVE FUSION                                         #############\n",
    "############################################################################################################\n",
    "\"\"\"\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Multiplicative fusion\n",
    "multiplicative_fused_features = Multiply()([unet_features_flat, lstm_features])\n",
    "\n",
    "# Add a dense layer for final classification\n",
    "x = BatchNormalization()(multiplicative_fused_features)\n",
    "\n",
    "# 2. Implement a deeper network with residual connections\n",
    "x1 = Dense(64, activation='relu')(x)\n",
    "x1 = Dropout(0.3)(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "\n",
    "# Second branch - keep same dimensions for residual connection\n",
    "x2 = Dense(64, activation='relu')(x1)\n",
    "x2 = Dropout(0.2)(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "\n",
    "# Now the shapes match for the residual connection\n",
    "x = tf.keras.layers.Add()([x1, x2])\n",
    "\n",
    "# Final classification layers\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "multiplicative_output = Dense(3, activation='softmax')(x)\n",
    "\n",
    "print(\"Model's output shape\", multiplicative_output.shape)\n",
    "\n",
    "# Create the fusion model\n",
    "multiplicative_fusion_model = Model(\n",
    "    inputs=[unet_input, lstm_input],\n",
    "    outputs=multiplicative_output\n",
    ")\n",
    "\n",
    "# Print model's expected input shape and output shape\n",
    "print(\"Expected input shape: \", multiplicative_fusion_model.input_shape)\n",
    "print(\"Expected output shape: \", multiplicative_fusion_model.output_shape)\n",
    "\n",
    "initial_learning_rate = 0.001\n",
    "optimizer = Adam(\n",
    "    learning_rate=initial_learning_rate,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07\n",
    ")\n",
    "\n",
    "\n",
    "def build_mlf_model(learning_rate=0.001, dropout1=0.3, dropout2=0.2, dropout3=0.1):\n",
    "    # Add a dense layer for final classification\n",
    "    x = BatchNormalization()(multiplicative_fused_features)\n",
    "\n",
    "    x1 = Dense(64, activation='relu')(x)\n",
    "    x1 = Dropout(dropout1)(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "\n",
    "    x2 = Dense(64, activation='relu')(x1)\n",
    "    x2 = Dropout(dropout2)(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "\n",
    "    x = tf.keras.layers.Add()([x1, x2])\n",
    "\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(dropout3)(x)\n",
    "    multiplicative_output = Dense(3, activation='softmax')(x)\n",
    "\n",
    "    # Create the fusion model\n",
    "    mlf_model = Model(\n",
    "        inputs=[unet_input, lstm_input],\n",
    "        outputs=multiplicative_output\n",
    "    )\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    mlf_model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy', \n",
    "            tf.keras.metrics.AUC(), \n",
    "            tf.keras.metrics.Precision(), \n",
    "            tf.keras.metrics.Recall()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return mlf_model\n",
    "\n",
    "# Compile the fusion model\n",
    "multiplicative_fusion_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.AUC(),\n",
    "        tf.keras.metrics.Precision(),\n",
    "        tf.keras.metrics.Recall()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# show the model architecture\n",
    "# plot_model(\n",
    "#     multiplicative_fusion_model, \n",
    "#     to_file='model.png', \n",
    "#     show_shapes=True, \n",
    "#     show_layer_names=False, \n",
    "#     rankdir='TB', \n",
    "#     expand_nested=False, \n",
    "#     dpi=96\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for additive fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's output shape (None, 3)\n",
      "Expected input shape:  [(None, 256, 256, 1), (None, 5, 4)]\n",
      "Expected output shape:  (None, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "############################################################################################################\n",
    "###########                      ADDITIVE FUSION                                         ###################        \n",
    "############################################################################################################\n",
    "\"\"\"\n",
    "\n",
    "# Additive fusion\n",
    "additive_fused_features = Concatenate()([unet_features_flat, lstm_features])\n",
    "\n",
    "# Add a dense layer for final classification\n",
    "\n",
    "x = BatchNormalization()(additive_fused_features)\n",
    "\n",
    "# 2. Implement a deeper network with residual connections\n",
    "x1 = Dense(64, activation='relu')(x)\n",
    "x1 = Dropout(0.3)(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "\n",
    "# Second branch - keep same dimensions for residual connection\n",
    "x2 = Dense(64, activation='relu')(x1)\n",
    "x2 = Dropout(0.2)(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "\n",
    "# Now the shapes match for the residual connection\n",
    "x = tf.keras.layers.Add()([x1, x2])\n",
    "\n",
    "# Final classification layers\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "additive_output = Dense(3, activation='softmax')(x)\n",
    "print(\"Model's output shape\", additive_output.shape)\n",
    "\n",
    "\n",
    "# Create the fusion model\n",
    "additive_fusion_model = Model(\n",
    "    inputs=[unet_input, lstm_input],\n",
    "    outputs=additive_output\n",
    ")\n",
    "\n",
    "# Print model's expected input shape and output shape\n",
    "print(\"Expected input shape: \", additive_fusion_model.input_shape)\n",
    "print(\"Expected output shape: \", additive_fusion_model.output_shape)\n",
    "\n",
    "initial_learning_rate = 0.001\n",
    "optimizer_af = Adam(\n",
    "    learning_rate=initial_learning_rate,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07\n",
    ")\n",
    "\n",
    "# Compile the fusion model\n",
    "additive_fusion_model.compile(\n",
    "    optimizer=optimizer_af,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.AUC(),\n",
    "        tf.keras.metrics.Precision(),\n",
    "        tf.keras.metrics.Recall()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# show the model architecture\n",
    "# plot_model(additive_fusion_model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for Gated fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's output shape (None, 3)\n",
      "Expected input shape:  [(None, 256, 256, 1), (None, 5, 4)]\n",
      "Expected output shape:  (None, 3)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "############################################################################################################\n",
    "###########                                 GATED FUSION                                       #############        \n",
    "############################################################################################################\n",
    "\"\"\"\n",
    "# Transform the lstm_features to match the shape of the unet_features_flat\n",
    "# lstm_features_transformed = Dense(unet_features_flat.shape[-1])(lstm_features)\n",
    "\n",
    "gate = Dense(unet_features_flat.shape[-1], activation='sigmoid')(Concatenate()([unet_features_flat, lstm_features]))\n",
    "\n",
    "# Apply the gate to the features\n",
    "gated_unet_features = Multiply()([unet_features_flat, gate])\n",
    "gated_lstm_features = Multiply()([lstm_features, 1 - gate])\n",
    "\n",
    "# Combine the gated features\n",
    "gated_fused_features = Concatenate()([gated_unet_features, gated_lstm_features])\n",
    "\n",
    "# Add a dense layer for final classification\n",
    "\n",
    "x = BatchNormalization()(gated_fused_features)\n",
    "\n",
    "# 2. Implement a deeper network with residual connections\n",
    "x1 = Dense(64, activation='relu')(x)\n",
    "x1 = Dropout(0.3)(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "\n",
    "# Second branch - keep same dimensions for residual connection\n",
    "x2 = Dense(64, activation='relu')(x1)\n",
    "x2 = Dropout(0.2)(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "\n",
    "# Now the shapes match for the residual connection\n",
    "x = tf.keras.layers.Add()([x1, x2])\n",
    "\n",
    "# Final classification layers\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "gated_output = Dense(3, activation='softmax')(x)\n",
    "print(\"Model's output shape\", gated_output.shape)\n",
    "\n",
    "# Create the fusion model\n",
    "gated_fusion_model = Model(\n",
    "    inputs=[unet_input, lstm_input],\n",
    "    outputs=gated_output\n",
    ")\n",
    "\n",
    "# Print model's expected input shape and output shape\n",
    "print(\"Expected input shape: \", gated_fusion_model.input_shape)\n",
    "print(\"Expected output shape: \", gated_fusion_model.output_shape)\n",
    "initial_learning_rate = 0.001\n",
    "optimizer_gf = Adam(\n",
    "    learning_rate=initial_learning_rate,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07\n",
    ")\n",
    "# Compile the fusion model\n",
    "gated_fusion_model.compile(\n",
    "    optimizer=optimizer_gf,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.AUC(),\n",
    "        tf.keras.metrics.Precision(),\n",
    "        tf.keras.metrics.Recall()\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final splitting of the data and visualizing the shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3267, 256, 256, 1)\n",
      "(3267, 256, 256, 1)\n",
      "(3267, 5, 4)\n",
      "(3267, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before categorical conversion\n",
      "X_train_lstm (2613, 5, 4)\n",
      "X_test_lstm (654, 5, 4)\n",
      "Y_train (2613, 3)\n",
      "Y_test (654, 3)\n",
      "masks_train (2613, 256, 256, 1)\n",
      "masks_test (654, 256, 256, 1)\n",
      "Class values in the dataset are ...  [0 1 2]\n",
      "##########\n",
      "After categorical conversion\n",
      "X_train_unet train image shape (2613, 256, 256, 1)\n",
      "X_test_unet train image shape (654, 256, 256, 1)\n",
      "y_train_cat shape (2613, 256, 256, 3)\n",
      "y_test_cat shape (654, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "# print sizes of all train_images, new_lstm_features, lstm_labels_multi\n",
    "print(train_images.shape) # (835, 256, 256, 1)\n",
    "print(train_masks_input.shape) # (835, 256, 256, 1) this is the label for the unet model\n",
    "print(new_lstm_features.shape) # (835, 5, 4)\n",
    "print(lstm_labels_multi.shape) # (835, 3)\n",
    "\n",
    "\n",
    "# (1632, 256, 256, 1)\n",
    "# (1632, 256, 256, 1)\n",
    "# (1616, 5, 4)\n",
    "# (1632, 3)\n",
    "\n",
    "X_train_unet, X_test_unet, X_train_lstm, X_test_lstm, Y_train, Y_test, masks_train, masks_test = train_test_split(\n",
    "    train_images, new_lstm_features, lstm_labels_multi, train_masks_input, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# print sizes of all X_train_unet, X_test_unet, X_train_lstm, X_test_lstm, Y_train, Y_test\n",
    "print(\"Before categorical conversion\")\n",
    "print(\"X_train_lstm\", X_train_lstm.shape) \n",
    "print(\"X_test_lstm\", X_test_lstm.shape)\n",
    "print(\"Y_train\", Y_train.shape) # (668, 3)\n",
    "print(\"Y_test\", Y_test.shape) # (167, 3)\n",
    "print(\"masks_train\", masks_train.shape)\n",
    "print(\"masks_test\", masks_test.shape)\n",
    "print(\"Class values in the dataset are ... \", np.unique(masks_train))\n",
    "\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "train_masks_cat = to_categorical(masks_train, num_classes=n_classes)\n",
    "y_train_cat = train_masks_cat.reshape((masks_train.shape[0], masks_train.shape[1], masks_train.shape[2], n_classes))\n",
    "\n",
    "\n",
    "test_masks_cat = to_categorical(masks_test, num_classes=n_classes)\n",
    "y_test_cat = test_masks_cat.reshape((masks_test.shape[0], masks_test.shape[1], masks_test.shape[2], n_classes))\n",
    "\n",
    "print(\"##########\")\n",
    "print(\"After categorical conversion\")\n",
    "print(\"X_train_unet train image shape\", X_train_unet.shape) # (668, 256, 256, 1)\n",
    "print(\"X_test_unet train image shape\", X_test_unet.shape) # (167, 256, 256, 1)\n",
    "print(\"y_train_cat shape\", y_train_cat.shape) # (668, 256, 256, 3)\n",
    "print(\"y_test_cat shape\", y_test_cat.shape) # (167, 256, 256, 3)\n",
    "\n",
    "# print(\"##############################################\")\n",
    "# print(f\"Shape of X_train_lstm: {X_train_lstm.shape}\")\n",
    "# print(f\"Shape of X_test_lstm: {X_test_lstm.shape}\")\n",
    "# # # Replace missing data for training set\n",
    "# X_train_lstm = replace_missing_with_identity(None, expected_shape=(X_train_lstm.shape[0], X_train_lstm.shape[-1]))\n",
    "# # # X_train_lstm = replace_missing_with_identity(X_train_lstm, (X_train_lstm.shape[0], lstm_features_dim))\n",
    "\n",
    "# # # Replace missing data for validation set\n",
    "# X_test_lstm = replace_missing_with_identity(None, expected_shape=(X_test_lstm.shape[0], X_test_lstm.shape[-1]))\n",
    "# # X_val_lstm = replace_missing_with_identity(X_val_lstm, (X_val_lstm.shape[0], lstm_features_dim))\n",
    "# print(f\"Shape of X_train_lstm: {X_train_lstm.shape}\")\n",
    "# print(f\"Shape of X_test_lstm: {X_test_lstm.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing GF with missing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make X_train_lstm and X_test_lstm to have an identity matrix without using the replace_missing_with_identity function\n",
    "X_train_unet_ = create_zero_padded_image_tensor(X_train_unet.shape)\n",
    "X_test_unet_ = create_zero_padded_image_tensor(X_test_unet.shape)\n",
    "print(f\"Shape of X_train_lstm: {X_train_lstm.shape}\")\n",
    "print(f\"Shape of X_test_lstm: {X_test_lstm.shape}\")\n",
    "print(f\"Shape of new X_train_lstm_: {X_train_lstm_.shape}\")\n",
    "print(f\"Shape of new X_test_lstm_: {X_test_lstm_.shape}\")\n",
    "\n",
    "history = train_fusion_model(\n",
    "    gated_fusion_model,\n",
    "    X_train_unet_,\n",
    "    X_train_lstm,\n",
    "    Y_train,\n",
    "    X_test_unet_,\n",
    "    X_test_lstm,\n",
    "    Y_test,\n",
    "    epochs=50\n",
    ")\n",
    "\n",
    "# Plot training\n",
    "plot_training_history(history)\n",
    "\n",
    "# # Evaluate the model\n",
    "evaluate_fusion_model(\n",
    "    gated_fusion_model,\n",
    "    X_test_unet_,\n",
    "    X_test_lstm,\n",
    "    Y_test\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Sizes of the training and testing sets\n",
    "# train_unet_size = X_train_unet.shape[0]\n",
    "# test_unet_size = X_test_unet.shape[0]\n",
    "# train_lstm_size = X_train_lstm.shape[0]\n",
    "# test_lstm_size = X_test_lstm.shape[0]\n",
    "\n",
    "# # Create a bar chart\n",
    "# labels = ['Train UNet', 'Test UNet', 'Train LSTM', 'Test LSTM']\n",
    "# sizes = [train_unet_size, test_unet_size, train_lstm_size, test_lstm_size]\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.bar(labels, sizes, color=['blue', 'orange', 'green', 'red'])\n",
    "# plt.xlabel('Dataset Split')\n",
    "# plt.ylabel('Number of Samples')\n",
    "# plt.title('Dataset Split Visualization')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the fusion models: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiplicative Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Train the fusion model with the UNet and LSTM features and considering the output of the LSTM and UNet models\n",
    "\"\"\"\n",
    "\n",
    "print(\"Given input shape: \", [X_train_unet.shape, X_train_lstm.shape])\n",
    "print(\"Given output shape: \", Y_train.shape)\n",
    "# Train the multiplicative fusion model\n",
    "\n",
    "history = train_fusion_model(\n",
    "    multiplicative_fusion_model,\n",
    "    X_train_unet,\n",
    "    X_train_lstm,\n",
    "    Y_train,\n",
    "    X_test_unet,\n",
    "    X_test_lstm,\n",
    "    Y_test,\n",
    "    epochs=50,\n",
    ")\n",
    "\n",
    "# Plot training\n",
    "plot_training_history(history)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_fusion_model(\n",
    "    multiplicative_fusion_model,\n",
    "    X_test_unet,\n",
    "    X_test_lstm,\n",
    "    Y_test\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additive Fusion (Concatenation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the additive fusion model\n",
    "history = train_fusion_model(\n",
    "    additive_fusion_model,\n",
    "    X_train_unet,\n",
    "    X_train_lstm,\n",
    "    Y_train,\n",
    "    X_test_unet,\n",
    "    X_test_lstm,\n",
    "    Y_test,\n",
    "    epochs=50\n",
    ")\n",
    "\n",
    "# Plot training\n",
    "plot_training_history(history)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_fusion_model(\n",
    "    additive_fusion_model,\n",
    "    X_test_unet,\n",
    "    X_test_lstm,\n",
    "    Y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique: Gated Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Train the fusion model with the UNet and LSTM features and considering the output of the LSTM and UNet models\n",
    "\"\"\"\n",
    "\n",
    "# print(\"Given input shape: \", [X_train_unet.shape, X_train_lstm.shape])\n",
    "# print(\"Given output shape: \", Y_train.shape)\n",
    "\n",
    "# Train the gated fusion model\n",
    "history = train_fusion_model(\n",
    "    gated_fusion_model,\n",
    "    X_train_unet,\n",
    "    X_train_lstm,\n",
    "    Y_train,\n",
    "    X_test_unet,\n",
    "    X_test_lstm,\n",
    "    Y_test,\n",
    "    epochs=50\n",
    ")\n",
    "\n",
    "# Plot training\n",
    "plot_training_history(history)\n",
    "\n",
    "# # Evaluate the model\n",
    "evaluate_fusion_model(\n",
    "    gated_fusion_model,\n",
    "    X_test_unet,\n",
    "    X_test_lstm,\n",
    "    Y_test\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
